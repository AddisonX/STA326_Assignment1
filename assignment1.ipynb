{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA326 Assignment 1: Web Scraping\n",
    "This is an assignment that is openly available for the Data Science Practice (STA326)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d4dd9ff1c43cbf5ebb01d9bc894c925",
     "grade": false,
     "grade_id": "overview",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "In this assignment, we will explore web scraping, which can often include diverse information from website, and also use the data for simple analysis. We take [douban](https://movie.douban.com/top250) as the target website in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eea06ff714fd3f00b5474b36d522ca50",
     "grade": false,
     "grade_id": "import-code",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests  # send request\n",
    "from bs4 import BeautifulSoup  # parse web pages\n",
    "import pandas as pd  # csv\n",
    "from time import sleep  # wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping\n",
    "\n",
    "### Scraping Rules\n",
    "\n",
    "1) If you are using another organization's website for scraping, make sure to check the website's terms & conditions. \n",
    "\n",
    "2) Do not request data from the website too aggressively (quickly) with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "\n",
    "3) The layout of a website may change from time to time. Because of this, if you're scraping a website, make sure to revisit the site and rewrite your code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Basic workflow\n",
    "\n",
    "We will first retrieve the contents on a page and examine them a bit.\n",
    "\n",
    "Make a variable called `page_url`, that stores the URL (as a string) like:\n",
    "https://movie.douban.com/top250?start=0\n",
    "\n",
    "Now, to open the URL, use `requests.get()` and provide `page_url` as its input. Store this in a variable called `page`.\n",
    "\n",
    "After that, make a variable called `soup` to parse the HTML using `BeautifulSoup`. Consider that there will be a method from `BeautifulSoup` that you'll need to call on to get the content from the page. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Extracting Data\n",
    "\n",
    "In order to extract the data we want, we’ll start with extracting a data list of interest.\n",
    "\n",
    "Extract the data from the page and save it in a variable (list) like `movie_name` and `movie_star`. \n",
    "\n",
    "Make sure you extract it as a string.\n",
    "\n",
    "We present a example to scrape `movie_name` and `movie_star` from `page_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['肖申克的救赎\\xa0/\\xa0The Shawshank Redemption\\xa0/\\xa0月黑高飞(港)  /  刺激1995(台)', '霸王别姬\\xa0/\\xa0再见，我的妾  /  Farewell My Concubine', '阿甘正传\\xa0/\\xa0Forrest Gump\\xa0/\\xa0福雷斯特·冈普', '泰坦尼克号\\xa0/\\xa0Titanic\\xa0/\\xa0铁达尼号(港 / 台)', '这个杀手不太冷\\xa0/\\xa0Léon\\xa0/\\xa0终极追杀令(台)  /  杀手莱昂', '千与千寻\\xa0/\\xa0千と千尋の神隠し\\xa0/\\xa0神隐少女(台)  /  千与千寻的神隐', '美丽人生\\xa0/\\xa0La vita è bella\\xa0/\\xa0一个快乐的传说(港)  /  Life Is Beautiful', '星际穿越\\xa0/\\xa0Interstellar\\xa0/\\xa0星际启示录(港)  /  星际效应(台)', '盗梦空间\\xa0/\\xa0Inception\\xa0/\\xa0潜行凶间(港)  /  全面启动(台)', \"辛德勒的名单\\xa0/\\xa0Schindler's List\\xa0/\\xa0舒特拉的名单(港)  /  辛德勒名单\", '楚门的世界\\xa0/\\xa0The Truman Show\\xa0/\\xa0真人Show(港)  /  真人戏', \"忠犬八公的故事\\xa0/\\xa0Hachi: A Dog's Tale\\xa0/\\xa0秋田犬八千(港)  /  忠犬小八(台)\", \"海上钢琴师\\xa0/\\xa0La leggenda del pianista sull'oceano\\xa0/\\xa0声光伴我飞(港)  /  一九零零的传奇\", '三傻大闹宝莱坞\\xa0/\\xa03 Idiots\\xa0/\\xa0三个傻瓜(台)  /  作死不离3兄弟(港)', '放牛班的春天\\xa0/\\xa0Les choristes\\xa0/\\xa0歌声伴我心(港)  /  唱诗班男孩', '机器人总动员\\xa0/\\xa0WALL·E\\xa0/\\xa0太空奇兵·威E(港)  /  瓦力(台)', '疯狂动物城\\xa0/\\xa0Zootopia\\xa0/\\xa0优兽大都会(港)  /  动物方城市(台)', '无间道\\xa0/\\xa0無間道\\xa0/\\xa0Infernal Affairs  /  Mou gaan dou', '控方证人\\xa0/\\xa0Witness for the Prosecution\\xa0/\\xa0雄才伟略  /  情妇', '大话西游之大圣娶亲\\xa0/\\xa0西遊記大結局之仙履奇緣\\xa0/\\xa0西游记完结篇仙履奇缘  /  齐天大圣西游记', '熔炉\\xa0/\\xa0도가니\\xa0/\\xa0无声呐喊(港)  /  漩涡', \"教父\\xa0/\\xa0The Godfather\\xa0/\\xa0Mario Puzo's The Godfather\", '触不可及\\xa0/\\xa0Intouchables\\xa0/\\xa0闪亮人生(港)  /  逆转人生(台)', '当幸福来敲门\\xa0/\\xa0The Pursuit of Happyness\\xa0/\\xa0寻找快乐的故事(港)  /  追求快乐', '寻梦环游记\\xa0/\\xa0Coco\\xa0/\\xa0玩转极乐园(港)  /  可可夜总会(台)']\n",
      "['9.7', '9.6', '9.5', '9.5', '9.4', '9.4', '9.5', '9.4', '9.4', '9.5', '9.4', '9.4', '9.3', '9.2', '9.3', '9.3', '9.2', '9.3', '9.6', '9.2', '9.4', '9.3', '9.3', '9.2', '9.1']\n"
     ]
    }
   ],
   "source": [
    "movie_name = []  # movie name\n",
    "movie_star = []  # movie star\n",
    "\n",
    "# Define a request header (to prevent anti-scraping)\n",
    "headers = {\n",
    "    'authority': 'curlconverter.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'if-modified-since': 'Fri, 15 Jul 2022 21:44:42 GMT',\n",
    "    'if-none-match': 'W/\"62d1dfca-3a58\"',\n",
    "    'referer': 'https://link.csdn.net/?target=https%3A%2F%2Fcurlconverter.com%2F',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Microsoft Edge\";v=\"102\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Linux\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30',\n",
    "}\n",
    "\n",
    "page_url = 'https://movie.douban.com/top250?start=0'\n",
    "\n",
    "res = requests.get(page_url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "for movie in soup.select('.item'):\n",
    "    name = movie.select('.hd a')[0].text.replace('\\n', '')  # select the movie name\n",
    "    movie_name.append(name)\n",
    "    star = movie.select('.rating_num')[0].text # select the movie star\n",
    "    movie_star.append(star)\n",
    "\n",
    "print(movie_name)\n",
    "print(movie_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) Collecting into a dataframe\n",
    "\n",
    "Create a dataframe `movie_df` and add the data from the lists above to it. \n",
    "- `movie_name` is the movie name. Set the column name as `movie name`\n",
    "- `movie_star` is the population estimate via star. Add it to the dataframe, and set the column name as `movie star`\n",
    "\n",
    "Make sure to check the head of your dataframe to see that everything looks right! ie: movie_df.head()\n",
    "\n",
    "We give an example to store the data as a text file (.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name =  \"MovieDouban.csv\"\n",
    "\n",
    "movie_df = pd.DataFrame() # initialize a DataFrame object\n",
    "movie_df['movie name'] = movie_name\n",
    "movie_df['movie star'] = movie_star\n",
    "\n",
    "movie_df.to_csv(csv_name, encoding='utf_8_sig')  # save data to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Extract DouBan's Top 250 Movies Information\n",
    "\n",
    "### Task Description\n",
    "Your task is to write two Python function. \n",
    "\n",
    "1. The first one named `get_movie_info` that scrapes the complete information of the top 250 movies listed on DouBan's movie ranking. The function should store the following details for each movie in the corresponding lists:\n",
    "\n",
    "- `movie_name`: List to store the names of the movies.\n",
    "- `movie_url`: List to store the URLs of the movies.\n",
    "- `movie_star`: List to store the ratings of the movies.\n",
    "- `movie_star_people`: List to store the number of people who have rated the movie.\n",
    "- `movie_director`: List to store the directors of the movies.\n",
    "- `movie_actor`: List to store the main actors of the movies.\n",
    "- `movie_year`: List to store the release year of the movies.\n",
    "- `movie_country`: List to store the country of origin of the movies.\n",
    "- `movie_type`: List to store the genre of the movies.\n",
    "\n",
    "\n",
    "2. The second function  `save_to_csv` function should return a dictionary `movie_df` where each key is one of the above categories, and the value is the list containing all the relevant information for the top 250 movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint\n",
    "\n",
    "While implementing the `get_movie_info` function, pay special attention to the following:\n",
    "\n",
    "1. **Director and Actor Information**: The webpage lists both directors and main actors for movies. Ensure that you correctly identify and separate these two pieces of information. In cases where the main actors are not mentioned, the `movie_actor` list should contain `None`.\n",
    "\n",
    "2. **Handling Movie with Multiple Years**: `'大闹天宫 / 大闹天宫 上下集 / The Monkey King'` have multiple release years listed together. You need to handle these cases appropriately to ensure that the `movie_year` list is populated correctly. A simple way is to link all possible years with a string for the mentioned movie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data here\n",
    "movie_name = []\n",
    "movie_url = [] \n",
    "movie_star = [] \n",
    "movie_star_people = []\n",
    "movie_director = [] \n",
    "movie_actor = []  \n",
    "movie_year = []  \n",
    "movie_country = []  \n",
    "movie_type = [] \n",
    "\n",
    "\n",
    "def get_movie_info(url, headers):\n",
    "    \"\"\"\n",
    "    Fetches detailed information about movies from DouBan\n",
    "    :param url: URL to scrape\n",
    "    :param headers: Headers for the scraping request\n",
    "    :return: A dictionary contains movie information\n",
    "    \"\"\"\n",
    "\n",
    "    ##### scrape movie info ###### \n",
    "    # YOUR CODE HERE\n",
    "    ##############################\n",
    "\n",
    "\n",
    "\n",
    "def save_to_csv(csv_name):\n",
    "\n",
    "    \"\"\"\n",
    "     save data to csv\n",
    "    :csv_name: Saved name\n",
    "    :return: A dictionary contains movie information\n",
    "    \"\"\"\n",
    "\n",
    "    movie_df = pd.DataFrame() # initialize a DataFrame\n",
    "\n",
    "    ##### save to dataframe ###### \n",
    "    # YOUR CODE HERE\n",
    "    ##############################\n",
    "\n",
    "\n",
    "    return movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Test\n",
    "\n",
    "You can test your implementation with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape page 1, URL: https://movie.douban.com/top250?start=0\n",
      "Starting to scrape page 2, URL: https://movie.douban.com/top250?start=25\n",
      "Starting to scrape page 3, URL: https://movie.douban.com/top250?start=50\n",
      "Starting to scrape page 4, URL: https://movie.douban.com/top250?start=75\n",
      "Starting to scrape page 5, URL: https://movie.douban.com/top250?start=100\n",
      "Starting to scrape page 6, URL: https://movie.douban.com/top250?start=125\n",
      "Starting to scrape page 7, URL: https://movie.douban.com/top250?start=150\n",
      "Starting to scrape page 8, URL: https://movie.douban.com/top250?start=175\n",
      "Starting to scrape page 9, URL: https://movie.douban.com/top250?start=200\n",
      "Starting to scrape page 10, URL: https://movie.douban.com/top250?start=225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a request header (to prevent anti-scraping)\n",
    "headers = {\n",
    "    'authority': 'curlconverter.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'if-modified-since': 'Fri, 15 Jul 2022 21:44:42 GMT',\n",
    "    'if-none-match': 'W/\"62d1dfca-3a58\"',\n",
    "    'referer': 'https://link.csdn.net/?target=https%3A%2F%2Fcurlconverter.com%2F',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Microsoft Edge\";v=\"102\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Linux\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30',\n",
    "}\n",
    "# Start scraping data from DouBan\n",
    "for i in range(10):  # Scrape 10 pages in total, 25 entries per page\n",
    "    page_url = 'https://movie.douban.com/top250?start={}'.format(str(i * 25))\n",
    "    print('Starting to scrape page {}, URL: {}'.format(str(i + 1), page_url))\n",
    "    get_movie_info(page_url, headers)\n",
    "    sleep(1)  # Wait for 1 second to prevent scraping protection\n",
    "\n",
    "# Save the data to a CSV file\n",
    "save_to_csv(csv_name=\"STA326_MovieDouban250.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f5f079ec0fc0fda0bdcb4d12cbbeb8d",
     "grade": false,
     "grade_id": "finish",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Complete!\n",
    "\n",
    "Congrats, you're done!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
